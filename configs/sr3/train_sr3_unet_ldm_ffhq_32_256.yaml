_import: configs/train_ref_config.yaml

boat:

  path: rhdiffusion.boats.conditioned_latent_diffusion_boat
  name: ConditionedLatentDiffusionBoat

  pretrained:
    latent_encoder:
      path: 'rhdiffusion.nn.wrappers.autoencoder_kl_wrapper'
      name: 'AutoencoderKLWrapper'
      pretrained: 'stabilityai/sd-vae-ft-mse'

  models: 
    net:
      path: 'rhdiffusion.nn.model_zoo.unet.generator'
      name: 'UNet2DModel'
      params:
        in_channels: $in_channels
        out_channels: $out_channels
        base_channels: $base_channels
        channel_mults: [1, 2, 4, 8]
        num_res_blocks: 2
        cond_in_channels: $in_channels
        groups: 8

  processors:
    preproc:
      path: rhcore.pipelines.chainer
      name: Chainer
      context_method: 'set_context'
      params:
        module_configs:
          - ref: 'pretrained.latent_encoder'
            args: ['encode']
          - ref: others.fmt

  others:
    fmt:
      path: rhdiffusion.nn.utils.item_to_list
      name: Item2List
      params:
        total_len: 4    # [1, 2, 4, 8]
        pos: 3          # append at the end, 32x32 -> 256x256

  losses: 
    net:
      path: 'rhopsr.nn.losses.weighted_loss'
      name: 'WeightedLoss'
      params:
        base_loss_fn_str: mse
      wrapper: # This wrapper is more complex needs parameters
        mpath: 'rhopsr.nn.losses.wrappers.list_of_keys.ListOfKeys'
        params:
          keys: ["prediction", "target", "weight"]

  samplers:
    net:
      path: 'rhdiffusion.samplers.ddim_sampler'
      name: 'DDIMSampler'         # Name of the model class to use
      config:
        beta_start: 0.0001
        beta_end: 0.02
        beta_schedule: "linear"
        steps_offset: 1
        clip_sample: false
        set_alpha_to_one: false
        prediction_type: "epsilon"
        num_train_timesteps: 1000
        eta: 0.0                   # Parameter controlling noise level in sampling
        num_inference_steps: 50    # Default sampling steps for inference

trainer: 
  devices: $devices
  max_epochs: $max_epochs
  val_check_epochs: 1
  state_save_epochs: 1
  fup_by_key: 
    net: true

_vars:

  devices: [0, 1, 2, 3, 4, 5, 6, 7]
  max_epochs: 500

  train_batch_size: 64
  valid_batch_size: 16
  num_workers: 16

  in_channels: 4
  out_channels: 4
  base_channels: 64

  train_folder_paths:
    gt: data/ffhq/ffhq_imgs/ffhq_256
    cond: data/ffhq/ffhq_imgs/ffhq_32
    
  valid_folder_paths:
    gt: data/celeba/subsets/celeba_256
    cond: data/celeba/subsets/celeba_32

  experiment_name: sr3_unet_ffhq_32_256

  ema_start: 0